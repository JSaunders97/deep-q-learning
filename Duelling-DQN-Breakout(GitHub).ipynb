{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Many imports\n",
    "import retro\n",
    "import numpy as np\n",
    "from queue import Queue\n",
    "import cv2\n",
    "import itertools\n",
    "from retro.examples.discretizer import Discretizer\n",
    "from IPython.display import clear_output, display\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import copy\n",
    "import time\n",
    "from matplotlib import pyplot as plt\n",
    "from operator import itemgetter \n",
    "import pickle\n",
    "import csv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env_Wrap:\n",
    "    def __init__(self, state_len, num_frame_skip):\n",
    "        #Initiate Environment Wrapper\n",
    "        self.env = retro.RetroEnv(game='Breakout-Atari2600')\n",
    "        \n",
    "        # Define action space in terms of valid combinations of button presses\n",
    "        combos = [['BUTTON'],['LEFT'],['RIGHT'],[]]\n",
    "        self.action_space_size = len(combos)\n",
    "        self.env = Discretizer(self.env, combos=combos)\n",
    "        \n",
    "        #Number of frames to skip\n",
    "        self.num_frame_skip = num_frame_skip \n",
    "        self.num_frame_skip_1 = num_frame_skip - 3\n",
    "        \n",
    "        #Number of frames in each state\n",
    "        self.state_len = state_len\n",
    "        \n",
    "        _ = self.reset()\n",
    "        \n",
    "    def reset(self):\n",
    "        #Frame buffer\n",
    "        self.frame_buffer = np.zeros((80,80,2))\n",
    "        \n",
    "        #Hold the previous 4 states\n",
    "        self.state_queue = Queue(maxsize = self.state_len)\n",
    "        #Fill state queue with empty states\n",
    "        for i in range(self.state_len):\n",
    "            self.state_queue.put(np.zeros((80,80)))\n",
    "        \n",
    "        self.env.reset()\n",
    "        #Take NOOP action \n",
    "        for _ in range(15):\n",
    "            state,_,_,_ = self.step(3)\n",
    "        \n",
    "        return state\n",
    "        \n",
    "    def step(self, action):\n",
    "        \n",
    "        next_step, reward, terminal, info = self.frame_skip(action)\n",
    "        _ = self.state_queue.get()\n",
    "        self.state_queue.put(next_step)\n",
    "        state = np.stack(list(self.state_queue.queue)).astype(np.uint8)\n",
    "        \n",
    "        return state, reward, terminal, info\n",
    "    \n",
    "    def render(self):\n",
    "        self.env.render()\n",
    "        \n",
    "    def close(self):\n",
    "        self.env.close()\n",
    "    \n",
    "    def frame_skip(self, action):\n",
    "        \n",
    "        total_reward = 0.0\n",
    "        for skip in range(self.num_frame_skip):\n",
    "        \n",
    "            if skip > self.num_frame_skip_1:\n",
    "                obs, rew, done, info = self.env.step(action)\n",
    "                self.frame_buffer[:,:,skip-(self.num_frame_skip-2)] = self.grayscale_downsample_crop(obs)\n",
    "            \n",
    "            else:\n",
    "                obs, rew, done, info = self.env.step(action)\n",
    "                \n",
    "            total_reward += rew\n",
    "        \n",
    "        max_pool = np.maximum(self.frame_buffer[:,:,0],self.frame_buffer[:,:,1])\n",
    "        \n",
    "        return max_pool, total_reward, done, info\n",
    "    \n",
    "    def grayscale_downsample_crop(self, frame):\n",
    "        #Frames are coverted to gray scale, downsampled by 50% and cropped to 80x80 \n",
    "        \n",
    "        gray_frame = cv2.cvtColor(frame, cv2.COLOR_RGB2GRAY)\n",
    "        \n",
    "        (oldh, oldw, oldc) = self.env.observation_space.shape\n",
    "        ratio = 2\n",
    "        newshape = (oldw//ratio, oldh//ratio)\n",
    "        \n",
    "        downsampled_gray_frame = cv2.resize(gray_frame, newshape, interpolation=cv2.INTER_AREA)\n",
    "        \n",
    "        grayscale_downsample_crop_frame = downsampled_gray_frame[20:100,:]\n",
    "        \n",
    "        return grayscale_downsample_crop_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run a random agent on the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Random_Agent():\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def select_action(self):\n",
    "        return env_wrap.env.action_space.sample()  # sample random action\n",
    "\n",
    "random_agent = Random_Agent()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 9 sum_of_rewards_for_episode: 0.0\n",
      "episode: 9 sum_of_rewards_for_episode: 0.0\n"
     ]
    }
   ],
   "source": [
    "N = 10\n",
    "max_time_steps = 200\n",
    "env_wrap = Env_Wrap(4,4)\n",
    "\n",
    "#run the random agent in the environment for N episodes\n",
    "for episode in range(N):\n",
    "    reward_sum = 0\n",
    "    next_state = env_wrap.env.reset()\n",
    "    for i in range(max_time_steps):\n",
    "        action = random_agent.select_action()\n",
    "        next_state, reward, terminal, info = env_wrap.step(action)\n",
    "        reward_sum += reward\n",
    "        #env_wrap.render()\n",
    "        if terminal:\n",
    "            clear_output(wait=True)\n",
    "            time.sleep(0.2)\n",
    "            print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum)\n",
    "            break\n",
    "print('episode:', episode, 'sum_of_rewards_for_episode:', reward_sum)\n",
    "#env_wrap.env.render(close=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Constructing the Q Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of observations for a given state: 4\n",
      "Number of actions for a given state: 4\n"
     ]
    }
   ],
   "source": [
    "state_dim = 4\n",
    "action_dim = env_wrap.env.action_space.n\n",
    "\n",
    "print('Number of observations for a given state:', state_dim)\n",
    "print('Number of actions for a given state:', action_dim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Larger network used in nature paper \n",
    "class Q_Network(nn.Module):\n",
    "    def __init__(self, state_dim , action_dim):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.x_layer = nn.Conv2d(state_dim,32,8,stride=4) #convolves 32 filters of 8 x 8 with stride 4\n",
    "        nn.init.kaiming_normal_(self.x_layer.weight, mode='fan_in', nonlinearity='relu') #He (2015) initialization\n",
    "        self.h_layer1 = nn.Conv2d(32,64,4,stride=2) #convolves 64 filters of 4 x 4 with stride 2\n",
    "        nn.init.kaiming_normal_(self.h_layer1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        self.h_layer2 = nn.Conv2d(64,64,3,stride=1) #convolves 64 filters of 3 x 3 with stride 1\n",
    "        nn.init.kaiming_normal_(self.h_layer2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        self.h_layer3 = nn.Linear(6*6*64,512) #512 rectifier units\n",
    "        nn.init.kaiming_normal_(self.h_layer3.weight, mode='fan_in', nonlinearity='relu')\n",
    "        self.y_layer = nn.Linear(512, action_dim) #fully-connected linear layer with a single output for each action\n",
    "\n",
    "    def forward(self, state):\n",
    "        state = state/255.0\n",
    "        xh = F.relu(self.x_layer(state))\n",
    "        hh1 = F.relu(self.h_layer1(xh))\n",
    "        hh2 = F.relu(self.h_layer2(hh1))\n",
    "        hh2 = hh2.view(-1,6*6*64)\n",
    "        hh3 = F.relu(self.h_layer3(hh2))\n",
    "        state_action_values = self.y_layer(hh3)\n",
    "        return state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Duelling network\n",
    "#could include different types of advantage value function too\n",
    "class Duelling_Q_Network(nn.Module):\n",
    "    def __init__(self, state_dim , action_dim):\n",
    "        super(Duelling_Q_Network, self).__init__()\n",
    "        self.x_layer = nn.Conv2d(state_dim,32,8,stride=4) #convolves 32 filters of 8 x 8 with stride 4\n",
    "        nn.init.kaiming_normal_(self.x_layer.weight, mode='fan_in', nonlinearity='relu') #He (2015) initialization\n",
    "        \n",
    "        self.h_layer1 = nn.Conv2d(32,64,4,stride=2) #convolves 64 filters of 4 x 4 with stride 2\n",
    "        nn.init.kaiming_normal_(self.h_layer1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "        self.h_layer2 = nn.Conv2d(64,64,3,stride=1) #convolves 64 filters of 3 x 3 with stride 1\n",
    "        nn.init.kaiming_normal_(self.h_layer2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "        self.advantage = nn.Linear(6*6*64,512) #512 rectifier units\n",
    "        nn.init.kaiming_normal_(self.advantage.weight, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "        self.value = nn.Linear(6*6*64,512) #512 rectifier units\n",
    "        nn.init.kaiming_normal_(self.value.weight, mode='fan_in', nonlinearity='relu')\n",
    "        \n",
    "        self.advantage_2 = nn.Linear(512, action_dim)     \n",
    "        self.value_2 = nn.Linear(512, 1)\n",
    "        \n",
    "\n",
    "    def forward(self, state):\n",
    "        state = state/255.0\n",
    "        xh = F.relu(self.x_layer(state))\n",
    "        hh1 = F.relu(self.h_layer1(xh))\n",
    "        hh2 = F.relu(self.h_layer2(hh1))\n",
    "        hh2 = hh2.view(-1,6*6*64)\n",
    "        \n",
    "        adv = F.relu(self.advantage(hh2))\n",
    "        val = F.relu(self.value(hh2))\n",
    "        \n",
    "        adv = self.advantage_2(adv)\n",
    "        val = self.value_2(val).expand(hh2.size(0), action_dim)\n",
    "        \n",
    "        state_action_values = val + adv - adv.mean(1).unsqueeze(1).expand(hh2.size(0), action_dim)\n",
    "        return state_action_values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nclass Q_Network(nn.Module):\\n    def __init__(self, state_dim , action_dim):\\n        super(Q_Network, self).__init__()\\n        self.x_layer = nn.Conv2d(state_dim,16,8,stride=4)\\n        nn.init.kaiming_normal_(self.x_layer.weight, mode='fan_in', nonlinearity='relu')\\n        self.h_layer1 = nn.Conv2d(16,32,4,stride=2)\\n        nn.init.kaiming_normal_(self.h_layer1.weight, mode='fan_in', nonlinearity='relu')\\n        self.h_layer2 = nn.Linear(8*8*32,256)\\n        nn.init.kaiming_normal_(self.h_layer2.weight, mode='fan_in', nonlinearity='relu')\\n        self.y_layer = nn.Linear(256, action_dim)\\n\\n    def forward(self, state):\\n        xh = F.relu(self.x_layer(state))\\n        hh1 = F.relu(self.h_layer1(xh))\\n        hh1 = hh1.view(-1,8*8*32)\\n        hh2 = F.relu(self.h_layer2(hh1))\\n        state_action_values = self.y_layer(hh2)\\n        return state_action_values\\n\""
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Smaller network used in Atari DQN paper\n",
    "'''\n",
    "class Q_Network(nn.Module):\n",
    "    def __init__(self, state_dim , action_dim):\n",
    "        super(Q_Network, self).__init__()\n",
    "        self.x_layer = nn.Conv2d(state_dim,16,8,stride=4)\n",
    "        nn.init.kaiming_normal_(self.x_layer.weight, mode='fan_in', nonlinearity='relu')\n",
    "        self.h_layer1 = nn.Conv2d(16,32,4,stride=2)\n",
    "        nn.init.kaiming_normal_(self.h_layer1.weight, mode='fan_in', nonlinearity='relu')\n",
    "        self.h_layer2 = nn.Linear(8*8*32,256)\n",
    "        nn.init.kaiming_normal_(self.h_layer2.weight, mode='fan_in', nonlinearity='relu')\n",
    "        self.y_layer = nn.Linear(256, action_dim)\n",
    "\n",
    "    def forward(self, state):\n",
    "        xh = F.relu(self.x_layer(state))\n",
    "        hh1 = F.relu(self.h_layer1(xh))\n",
    "        hh1 = hh1.view(-1,8*8*32)\n",
    "        hh2 = F.relu(self.h_layer2(hh1))\n",
    "        state_action_values = self.y_layer(hh2)\n",
    "        return state_action_values\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output of our q_network given a state input:  tensor([[-0.1677, -0.0511,  0.2887,  0.7061]], grad_fn=<SubBackward0>)\n"
     ]
    }
   ],
   "source": [
    "#Test network gives the correct output\n",
    "qnet = Duelling_Q_Network(state_dim, action_dim)\n",
    "state = torch.from_numpy(env_wrap.reset()).float().view(-1,4,80,80)\n",
    "q_values = qnet(state)\n",
    "print('output of our q_network given a state input: ', q_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Construct the Replay Buffer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReplayBuffer(object):\n",
    "    def __init__(self, buffer_len):\n",
    "        self.buffer_len = buffer_len\n",
    "        self.buffer_states = []\n",
    "        self.buffer_next_states = []\n",
    "        self.buffer_actions = []\n",
    "        self.buffer_rewards = []\n",
    "        self.buffer_terminals = []\n",
    "        self.buffer_size = 0\n",
    "        self.cur_pos = 0\n",
    "\n",
    "    def add_to_buffer(self, data):\n",
    "        #data must be of the form (state,next_state,action,reward,terminal)\n",
    "        \n",
    "        if self.buffer_size < self.buffer_len:\n",
    "            self.buffer_size += 1\n",
    "            self.buffer_states.append(data[0])\n",
    "            self.buffer_next_states.append(data[1])\n",
    "            self.buffer_actions.append(data[2]) \n",
    "            self.buffer_rewards.append(data[3])\n",
    "            self.buffer_terminals.append(data[4]) \n",
    "        \n",
    "        else:\n",
    "            self.buffer_states[self.cur_pos] = data[0]\n",
    "            self.buffer_next_states[self.cur_pos] = data[1]\n",
    "            self.buffer_actions[self.cur_pos] = data[2]\n",
    "            self.buffer_rewards[self.cur_pos] = data[3]\n",
    "            self.buffer_terminals[self.cur_pos] = data[4]\n",
    "            self.cur_pos = (self.cur_pos + 1) % self.buffer_len\n",
    "        \n",
    "    def sample_minibatch(self,minibatch_length):\n",
    "        \n",
    "        if minibatch_length > self.buffer_size:\n",
    "            #Return entire buffer\n",
    "            return torch.FloatTensor(self.buffer_states).cuda(), \\\n",
    "                    torch.FloatTensor(self.buffer_next_states).cuda(), \\\n",
    "                    torch.FloatTensor(self.buffer_actions).cuda(), \\\n",
    "                    torch.FloatTensor(self.buffer_rewards).cuda(), \\\n",
    "                    torch.FloatTensor(self.buffer_terminals).cuda()\n",
    "        \n",
    "        else:\n",
    "            \n",
    "            samples_ind = np.random.randint(0,self.buffer_size,minibatch_length)\n",
    "        \n",
    "            return torch.FloatTensor(itemgetter(*samples_ind)(self.buffer_states)).cuda(), \\\n",
    "                    torch.FloatTensor(itemgetter(*samples_ind)(self.buffer_next_states)).cuda(), \\\n",
    "                    torch.FloatTensor(itemgetter(*samples_ind)(self.buffer_actions)).cuda(), \\\n",
    "                    torch.FloatTensor(itemgetter(*samples_ind)(self.buffer_rewards)).cuda(), \\\n",
    "                    torch.FloatTensor(itemgetter(*samples_ind)(self.buffer_terminals)).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CUDAReplayBuffer(object):\n",
    "    def __init__(self, buffer_length):\n",
    "        self.buffer_length = buffer_length\n",
    "        gpu = torch.cuda.current_device()\n",
    "        # Preallocate buffer memory on the GPU\n",
    "        self.buffer_states = torch.zeros([buffer_length, 4, 80, 80], dtype=torch.uint8, device=gpu)\n",
    "        self.buffer_next_states = torch.zeros([buffer_length, 4, 80, 80], dtype=torch.uint8, device=gpu)\n",
    "        self.buffer_actions = torch.zeros([buffer_length, 1], dtype=torch.uint8, device=gpu)\n",
    "        self.buffer_rewards = torch.zeros([buffer_length, 1], dtype=torch.float32, device=gpu)\n",
    "        self.buffer_terminals = torch.zeros([buffer_length, 1], dtype=torch.float32, device=gpu)\n",
    "        self.nb_stored_items = 0\n",
    "        self.current_position = 0\n",
    "    def add_to_buffer(self, data):\n",
    "        # Write at current position\n",
    "        i = self.current_position\n",
    "        self.buffer_states[i,:,:,:] = torch.from_numpy(data[0])\n",
    "        self.buffer_next_states[i,:,:,:] = torch.from_numpy(data[1])\n",
    "        self.buffer_actions[i, 0] = torch.from_numpy(np.array(data[2], dtype=np.uint8).flatten())\n",
    "        self.buffer_rewards[i, 0] = torch.from_numpy(np.array(data[3], dtype=np.float32).flatten())\n",
    "        self.buffer_terminals[i, 0] = torch.from_numpy(np.array(data[4], dtype=np.float32).flatten())\n",
    "        # If buffer is not already full, increment nb_stored_items\n",
    "        if self.nb_stored_items < self.buffer_length:\n",
    "            self.nb_stored_items += 1\n",
    "        # Increment current write position\n",
    "        if self.current_position < self.buffer_length - 1:\n",
    "            self.current_position += 1\n",
    "        else:\n",
    "            self.current_position = 0\n",
    "    def sample_minibatch(self, minibatch_length):\n",
    "        if minibatch_length > self.nb_stored_items:\n",
    "            # If we are asked to sample more items than we have stored, just return everything that we\n",
    "            # have stored instead.\n",
    "            N = self.nb_stored_items\n",
    "            return self.buffer_states[:N,:,:,:], self.buffer_next_states[:N,:,:,:], \\\n",
    "                   self.buffer_actions[:N,:], self.buffer_rewards[:N,:], self.buffer_terminals[:N,:]\n",
    "        else:\n",
    "            # Otherwise, return a random subset of length `minibatch_length` from the stored items.\n",
    "            ids = np.random.randint(0, self.nb_stored_items, minibatch_length)\n",
    "            return self.buffer_states[ids,:,:,:], self.buffer_next_states[ids,:,:,:], \\\n",
    "                   self.buffer_actions[ids,:], self.buffer_rewards[ids,:], self.buffer_terminals[ids,:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DQN Agent Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQNAgent(object):\n",
    "    def __init__(self, state_dim, action_dim, gamma, buffer_size, tau, learning_rate, minibatch_size):\n",
    "        self.qnet = Duelling_Q_Network(state_dim, action_dim)\n",
    "        self.qnet_target = copy.deepcopy(self.qnet)\n",
    "        self.discount_factor = gamma\n",
    "        #self.MSELoss_function = nn.MSELoss()\n",
    "        self.MSELoss_function = nn.SmoothL1Loss()\n",
    "        self.replay_buffer = CUDAReplayBuffer(buffer_size)\n",
    "        self.tau = tau\n",
    "        self.minibatch_size = minibatch_size\n",
    "        \n",
    "        if torch.cuda.is_available():\n",
    "            self.qnet.cuda()\n",
    "            self.qnet_target.cuda()\n",
    "            print('Running model on GPU:',torch.cuda.get_device_name(torch.cuda.current_device()),'!')\n",
    "        \n",
    "        self.qnet_optim = torch.optim.Adam( self.qnet.parameters(), lr=learning_rate)\n",
    "    \n",
    "    def soft_target_update(self,network,target_network,tau):\n",
    "        for net_params, target_net_params in zip(network.parameters(), target_network.parameters()):\n",
    "            target_net_params.data.copy_(net_params.data * tau + target_net_params.data * (1 - tau))\n",
    "    \n",
    "    def epsilon_greedy_action(self, state, epsilon):\n",
    "        if np.random.uniform(0, 1) < epsilon:\n",
    "            return env_wrap.env.action_space.sample()  # choose random action\n",
    "        else:\n",
    "            network_output_to_numpy = self.qnet(state).cpu().data.numpy()\n",
    "            #network_output = self.qnet(state)\n",
    "            return np.argmax(network_output_to_numpy)  # choose greedy action\n",
    "            \n",
    "    def update_Q_Network(self, state, next_state, action, reward, terminals):\n",
    "        qsa = torch.gather(self.qnet(state), dim=1, index=action.long())\n",
    "        qsa_next_action = self.qnet_target(next_state)\n",
    "        qsa_next_action_max,_ = torch.max(qsa_next_action, dim=1, keepdim=True)\n",
    "        not_terminals = 1 - terminals\n",
    "        qsa_next_target = reward + not_terminals * self.discount_factor * qsa_next_action_max\n",
    "        q_network_loss = self.MSELoss_function(qsa, qsa_next_target.detach())\n",
    "        self.qnet_optim.zero_grad()\n",
    "        q_network_loss.backward()\n",
    "        #Gradient Clipping between (-1, 1)\n",
    "        for param in self.qnet.parameters():\n",
    "            param.grad.data.clamp_(-1, 1)\n",
    "        self.qnet_optim.step()\n",
    "        \n",
    "        return q_network_loss\n",
    "        \n",
    "    def update(self):\n",
    "        states, next_states, actions, rewards, terminals = self.replay_buffer.sample_minibatch(self.minibatch_size)\n",
    "        loss = self.update_Q_Network(states, next_states, actions, rewards, terminals)\n",
    "        #self.soft_target_update(self.qnet, self.qnet_target, self.tau)\n",
    "        \n",
    "        return loss.item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current episode: 6695 \n",
      "Current episode reward: 63.0 \n",
      "Average loss: 0.03007198849786073 \n",
      "Current total steps: 4999284 \n",
      "Greatest reward: 306.0 \n",
      "Current epsilon: 0.010022132653839201\n"
     ]
    }
   ],
   "source": [
    "#Learning parameters\n",
    "total_time_steps = 5000000 #Number of steps to train on\n",
    "max_time_steps = 10000 #Max number of step in each episode if terminal state not reached\n",
    "gamma = 0.99 #Discount factor\n",
    "buffer_size = 100000 #Number of transitions to store in replay memory, 10% total steps\n",
    "minibatch_size = 32 #Number of samples in minibatch update, 32 used in atari DQN paper\n",
    "tau = 0.001 #Soft target network update\n",
    "learning_rate = 0.00025 #Gradient step size\n",
    "target_update = 10000 #Target network update frequency (number of steps)\n",
    "learning_starts = 5000 #Model starts learning after x transitions \n",
    "update_freq = 4 #Minibatch update frequency in number of steps\n",
    "\n",
    "#Epsilon decay parameters\n",
    "epsilon_start = 1.0 #Starting epsilon value\n",
    "epsilon_mid = 0.1 #Epsion value after some training\n",
    "epsilon_final = 0.01 #Final epsiolon value\n",
    "epsilon_decay_length1 = 100000 #total_time_steps//10 #Number of episodes to decay epsilon over, 10% total time steps\n",
    "epsilon_decay_length2 = total_time_steps - epsilon_decay_length1\n",
    "eps = epsilon_start\n",
    "\n",
    "agent = DQNAgent(state_dim, action_dim, gamma, buffer_size, tau, learning_rate, minibatch_size)\n",
    "\n",
    "#Loop variables\n",
    "total_steps = 0\n",
    "episode = 0\n",
    "save_freq = 100000 #Save checkpoint every x steps\n",
    "best_reward = -np.inf\n",
    "\n",
    "\n",
    "#Loss moving average\n",
    "avg_loss = 0\n",
    "loss_list_len = 1000\n",
    "loss_list = [0.0]*loss_list_len\n",
    "loss_pos = 0\n",
    "\n",
    "rewards = []\n",
    "rewards_per_ep = []\n",
    "losses = []\n",
    "losses_per_ep = []\n",
    "\n",
    "while total_steps < total_time_steps:\n",
    "    episode += 1\n",
    "    lives = 5\n",
    "    state = env_wrap.reset()\n",
    "    clear_output(wait=True)\n",
    "    print('Current episode:',episode,\n",
    "          '\\nCurrent episode reward:', reward_sum,\n",
    "          '\\nAverage loss:', avg_loss,\n",
    "          '\\nCurrent total steps:',total_steps, \n",
    "          '\\nGreatest reward:', best_reward,\n",
    "          '\\nCurrent epsilon:', eps)\n",
    "    reward_sum = 0\n",
    "\n",
    "    for i in range(max_time_steps):\n",
    "        \n",
    "        total_steps += 1\n",
    "        action = agent.epsilon_greedy_action( torch.from_numpy(state).float().view(-1,4,80,80).cuda() , eps)\n",
    "        next_state, reward, terminal, info = env_wrap.step(action)\n",
    "        \n",
    "        if lives != info['lives']:\n",
    "            agent.replay_buffer.add_to_buffer( (state,next_state,[action],[reward],[True]) )\n",
    "            lives = info['lives']\n",
    "        else:\n",
    "            agent.replay_buffer.add_to_buffer( (state,next_state,[action],[reward],[terminal]) )\n",
    "        \n",
    "        state = next_state\n",
    "        \n",
    "        reward_sum += reward\n",
    "        \n",
    "        #Update network if learning has started and update frequency reached\n",
    "        if total_steps > learning_starts and total_steps % update_freq == 0:\n",
    "            loss = agent.update()\n",
    "            loss_list[loss_pos] = loss\n",
    "            loss_pos = (loss_pos + 1)%loss_list_len\n",
    "            avg_loss = sum(loss_list)/loss_list_len\n",
    "            losses.append(loss)\n",
    "        \n",
    "        #Update target network if target update freqency reached\n",
    "        if total_steps % target_update == 0:\n",
    "            agent.qnet_target.load_state_dict(agent.qnet.state_dict())\n",
    "            \n",
    "        #Decay epsilon if total steps is less than decay length\n",
    "        if epsilon_decay_length1 > total_steps:\n",
    "            eps -= (epsilon_start-epsilon_mid)/epsilon_decay_length1\n",
    "        else:\n",
    "            eps -= (epsilon_mid-epsilon_final)/epsilon_decay_length2    \n",
    "        \n",
    "        rewards.append(reward)\n",
    "        \n",
    "        #Periodically save the model incase crash\n",
    "        if total_steps % save_freq == 0:\n",
    "            torch.save({\n",
    "            'epoch': total_steps,\n",
    "            'model_state_dict': agent.qnet.state_dict(),\n",
    "            'optimizer_state_dict': agent.qnet_optim.state_dict()\n",
    "            }, 'd:/madge/checkpoint_%d.pth'%total_steps)\n",
    "            \n",
    "            # write rewards to file \n",
    "            with open('d:/madge/rewards', 'a+', newline='') as write_obj:\n",
    "                csv_writer = csv.writer(write_obj)\n",
    "                for i in range(len(rewards)):\n",
    "                    csv_writer.writerow([rewards[i]])\n",
    "            \n",
    "            # write losses to file\n",
    "            with open('d:/madge/losses', 'a+', newline='') as write_obj:\n",
    "                csv_writer = csv.writer(write_obj)\n",
    "                for i in range(len(losses)):\n",
    "                    csv_writer.writerow([losses[i]])\n",
    "\n",
    "            rewards = []\n",
    "            losses = []\n",
    "        \n",
    "        if terminal:\n",
    "            #clear_output(wait=True)\n",
    "            #print('Current total steps:', total_steps, 'Rewards for episode:', reward_sum)\n",
    "            break\n",
    "            \n",
    "    if reward_sum > best_reward:\n",
    "        best_reward = reward_sum\n",
    "    \n",
    "    rewards_per_ep.append(reward_sum)\n",
    "    losses_per_ep.append(avg_loss)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('d:/madge/rewards', 'a+', newline='') as write_obj:\n",
    "    csv_writer = csv.writer(write_obj)\n",
    "    for i in range(len(rewards)):\n",
    "        csv_writer.writerow([rewards[i]])\n",
    "            \n",
    "# write rewards per episode to file\n",
    "with open('d:/madge/rewards_per_ep', 'a+', newline='') as write_obj:\n",
    "    csv_writer = csv.writer(write_obj)\n",
    "    for i in range(len(rewards_per_ep)):\n",
    "        csv_writer.writerow([rewards_per_ep[i]])\n",
    "            \n",
    "# write losses to file\n",
    "with open('d:/madge/losses', 'a+', newline='') as write_obj:\n",
    "    csv_writer = csv.writer(write_obj)\n",
    "    for i in range(len(losses)):\n",
    "        csv_writer.writerow([losses[i]])\n",
    "                    \n",
    "# write average loss per episode to file\n",
    "with open('d:/madge/losses_per_ep', 'a+', newline='') as write_obj:\n",
    "    csv_writer = csv.writer(write_obj)\n",
    "    for i in range(len(losses_per_ep)):\n",
    "        csv_writer.writerow([losses_per_ep[i]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Save model\n",
    "torch.save(agent.qnet.state_dict(), 'DQN_model.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
